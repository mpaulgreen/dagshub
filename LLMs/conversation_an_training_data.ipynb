{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_section"
   },
   "source": [
    "# \ud83d\udcda Agent Conversation Data & Training Dataset Management with DagsHub\n",
    "\n",
    "This notebook implements **conversation data storage and training dataset management** using DagsHub, completing the third part of the agent development process.\n",
    "\n",
    "## \ud83c\udfaf What You'll Learn:\n",
    "- Store and version conversation histories\n",
    "- Create structured training datasets from conversations\n",
    "- Implement data versioning with DagsHub\n",
    "- Build dataset pipelines for agent improvement\n",
    "- Track dataset metrics and quality\n",
    "\n",
    "## \ud83d\udd17 Integration with Previous Work:\n",
    "- Uses the same DagsHub repository setup\n",
    "- Extends agent performance tracking with data storage\n",
    "- Provides datasets for agent fine-tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## \ud83d\ude80 Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_dependencies",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d6be8d32-81e1-416e-eb5a-edced3a24a40"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 All packages installed successfully!\n",
      "\ud83d\udcda Data management libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q dagshub[jupyter] mlflow openai python-dotenv\n",
    "!pip install -q pandas numpy jsonlines pyarrow\n",
    "!pip install -q datasets huggingface_hub\n",
    "!pip install -q dvc[s3] dvc-data\n",
    "\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "import uuid\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import pickle\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DagsHub and MLflow\n",
    "import dagshub\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# OpenAI\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "print(\"\u2705 All packages installed successfully!\")\n",
    "print(\"\ud83d\udcda Data management libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "credentials_section"
   },
   "source": [
    "## \ud83d\udd10 Configure Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "configure_credentials",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "outputId": "c208d9c0-ba2e-4a31-e039-3250f94c7af0"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Accessing as mpaul\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as mpaul\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:dagshub.auth.tokens:The added token already exists in the token cache, skipping\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Repository conversation_and_datasets doesn't exist, creating it under current user.\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository conversation_and_datasets doesn't exist, creating it under current user.\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"mpaul/conversation_and_datasets\"\u001b[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"mpaul/conversation_and_datasets\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Repository mpaul/conversation_and_datasets initialized!\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository mpaul/conversation_and_datasets initialized!\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Connected to DagsHub repository: mpaul/conversation_and_datasets\n",
      "\ud83d\udcca MLflow tracking URI: https://dagshub.com/mpaul/conversation_and_datasets.mlflow\n"
     ]
    }
   ],
   "source": [
    "# **Enter your credentials (same as previous notebooks):**\n",
    "\n",
    "# Repository name:\n",
    "REPO_NAME = \"conversation_and_datasets\"  \n",
    "\n",
    "# DagsHub username:\n",
    "USER_NAME = \"mpaul\"  \n",
    "\n",
    "# Email:\n",
    "EMAIL = \"mpaul@redhat.com\"  \n",
    "\n",
    "# OpenAI API key:\n",
    "OPENAI_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Initialize DagsHub\n",
    "dagshub.auth.add_app_token(token=dagshub.auth.get_token())\n",
    "dagshub.init(repo_name=REPO_NAME, repo_owner=USER_NAME)\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(f\"\u2705 Connected to DagsHub repository: {USER_NAME}/{REPO_NAME}\")\n",
    "print(f\"\ud83d\udcca MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conversation_storage_section"
   },
   "source": [
    "## \ud83d\udcac Conversation Data Storage Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "conversation_storage_class",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "55920aaf-06af-4708-b6a8-331992343fff"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Conversation Storage Framework initialized\n"
     ]
    }
   ],
   "source": [
    "class ConversationStorage:\n",
    "    \"\"\"Manages storage and retrieval of conversation data\"\"\"\n",
    "\n",
    "    def __init__(self, storage_path: str = \"conversation_data\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(exist_ok=True)\n",
    "        self.conversations = []\n",
    "        self.metadata = {}\n",
    "\n",
    "    def create_conversation_id(self) -> str:\n",
    "        \"\"\"Generate unique conversation ID\"\"\"\n",
    "        return f\"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "    def store_conversation(self,\n",
    "                         messages: List[Dict[str, str]],\n",
    "                         agent_config: Dict[str, Any],\n",
    "                         performance_metrics: Optional[Dict[str, Any]] = None,\n",
    "                         tags: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Store a conversation with metadata\"\"\"\n",
    "\n",
    "        conversation_id = self.create_conversation_id()\n",
    "\n",
    "        conversation_data = {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"messages\": messages,\n",
    "            \"agent_config\": agent_config,\n",
    "            \"performance_metrics\": performance_metrics or {},\n",
    "            \"tags\": tags or [],\n",
    "            \"message_count\": len(messages),\n",
    "            \"total_tokens\": sum(m.get(\"tokens\", 0) for m in messages),\n",
    "            \"duration_seconds\": performance_metrics.get(\"duration\", 0) if performance_metrics else 0\n",
    "        }\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        conversation_data[\"avg_message_length\"] = np.mean([len(m.get(\"content\", \"\")) for m in messages])\n",
    "        conversation_data[\"user_messages\"] = sum(1 for m in messages if m.get(\"role\") == \"user\")\n",
    "        conversation_data[\"assistant_messages\"] = sum(1 for m in messages if m.get(\"role\") == \"assistant\")\n",
    "\n",
    "        # Store to file\n",
    "        file_path = self.storage_path / f\"{conversation_id}.json\"\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(conversation_data, f, indent=2)\n",
    "\n",
    "        # Add to memory\n",
    "        self.conversations.append(conversation_data)\n",
    "\n",
    "        return conversation_id\n",
    "\n",
    "    def batch_store_conversations(self, conversations: List[Dict]) -> List[str]:\n",
    "        \"\"\"Store multiple conversations at once\"\"\"\n",
    "        conversation_ids = []\n",
    "\n",
    "        for conv in conversations:\n",
    "            conv_id = self.store_conversation(\n",
    "                messages=conv.get(\"messages\", []),\n",
    "                agent_config=conv.get(\"agent_config\", {}),\n",
    "                performance_metrics=conv.get(\"performance_metrics\"),\n",
    "                tags=conv.get(\"tags\")\n",
    "            )\n",
    "            conversation_ids.append(conv_id)\n",
    "\n",
    "        return conversation_ids\n",
    "\n",
    "    def load_conversation(self, conversation_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Load a specific conversation\"\"\"\n",
    "        file_path = self.storage_path / f\"{conversation_id}.json\"\n",
    "\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "\n",
    "    def export_to_jsonl(self, output_file: str = \"conversations.jsonl\"):\n",
    "        \"\"\"Export all conversations to JSONL format\"\"\"\n",
    "        output_path = self.storage_path / output_file\n",
    "\n",
    "        with jsonlines.open(output_path, mode='w') as writer:\n",
    "            for conv in self.conversations:\n",
    "                writer.write(conv)\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about stored conversations\"\"\"\n",
    "        if not self.conversations:\n",
    "            return {}\n",
    "\n",
    "        df = pd.DataFrame(self.conversations)\n",
    "\n",
    "        stats = {\n",
    "            \"total_conversations\": len(self.conversations),\n",
    "            \"total_messages\": df[\"message_count\"].sum(),\n",
    "            \"avg_messages_per_conversation\": df[\"message_count\"].mean(),\n",
    "            \"total_tokens\": df[\"total_tokens\"].sum(),\n",
    "            \"avg_tokens_per_conversation\": df[\"total_tokens\"].mean(),\n",
    "            \"unique_agent_configs\": len(df[\"agent_config\"].apply(json.dumps).unique()),\n",
    "            \"date_range\": {\n",
    "                \"start\": df[\"timestamp\"].min(),\n",
    "                \"end\": df[\"timestamp\"].max()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "print(\"\u2705 Conversation Storage Framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_builder_section"
   },
   "source": [
    "## \ud83c\udfd7\ufe0f Training Dataset Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dataset_builder_class",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4848e649-4f73-4b23-874c-d51ee0234d83"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Training Dataset Builder initialized\n"
     ]
    }
   ],
   "source": [
    "class TrainingDatasetBuilder:\n",
    "    \"\"\"Build training datasets from conversation data\"\"\"\n",
    "\n",
    "    def __init__(self, conversation_storage: ConversationStorage):\n",
    "        self.storage = conversation_storage\n",
    "        self.datasets = {}\n",
    "\n",
    "    def create_instruction_dataset(self,\n",
    "                                 min_quality_score: float = 0.7,\n",
    "                                 format_type: str = \"alpaca\") -> Dataset:\n",
    "        \"\"\"Create instruction-following dataset from high-quality conversations\"\"\"\n",
    "\n",
    "        instruction_data = []\n",
    "\n",
    "        for conv in self.storage.conversations:\n",
    "            # Filter by quality score if available\n",
    "            quality_score = conv.get(\"performance_metrics\", {}).get(\"quality_score\", 1.0)\n",
    "            if quality_score < min_quality_score:\n",
    "                continue\n",
    "\n",
    "            messages = conv[\"messages\"]\n",
    "\n",
    "            # Extract instruction-response pairs\n",
    "            for i in range(0, len(messages) - 1, 2):\n",
    "                if i + 1 < len(messages):\n",
    "                    user_msg = messages[i]\n",
    "                    assistant_msg = messages[i + 1]\n",
    "\n",
    "                    if user_msg.get(\"role\") == \"user\" and assistant_msg.get(\"role\") == \"assistant\":\n",
    "\n",
    "                        if format_type == \"alpaca\":\n",
    "                            # Alpaca format\n",
    "                            data_point = {\n",
    "                                \"instruction\": user_msg.get(\"content\", \"\"),\n",
    "                                \"input\": \"\",  # Can be enriched with context\n",
    "                                \"output\": assistant_msg.get(\"content\", \"\"),\n",
    "                                \"conversation_id\": conv[\"conversation_id\"],\n",
    "                                \"agent_config\": json.dumps(conv[\"agent_config\"]),\n",
    "                                \"quality_score\": quality_score\n",
    "                            }\n",
    "                        elif format_type == \"chatgpt\":\n",
    "                            # ChatGPT format\n",
    "                            data_point = {\n",
    "                                \"messages\": [\n",
    "                                    {\"role\": \"system\", \"content\": conv[\"agent_config\"].get(\"system_prompt\", \"\")},\n",
    "                                    {\"role\": \"user\", \"content\": user_msg.get(\"content\", \"\")},\n",
    "                                    {\"role\": \"assistant\", \"content\": assistant_msg.get(\"content\", \"\")}\n",
    "                                ],\n",
    "                                \"conversation_id\": conv[\"conversation_id\"],\n",
    "                                \"quality_score\": quality_score\n",
    "                            }\n",
    "                        else:\n",
    "                            # Simple format\n",
    "                            data_point = {\n",
    "                                \"prompt\": user_msg.get(\"content\", \"\"),\n",
    "                                \"response\": assistant_msg.get(\"content\", \"\"),\n",
    "                                \"conversation_id\": conv[\"conversation_id\"],\n",
    "                                \"quality_score\": quality_score\n",
    "                            }\n",
    "\n",
    "                        instruction_data.append(data_point)\n",
    "\n",
    "        # Create HuggingFace Dataset\n",
    "        dataset = Dataset.from_list(instruction_data)\n",
    "\n",
    "        # Add metadata\n",
    "        dataset.info.description = f\"Instruction dataset created from {len(self.storage.conversations)} conversations\"\n",
    "        dataset.info.version = \"1.0.0\"\n",
    "        dataset.info.license = \"apache-2.0\"\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_preference_dataset(self, comparison_pairs: List[Tuple[str, str]]) -> Dataset:\n",
    "        \"\"\"Create preference dataset for RLHF training\"\"\"\n",
    "\n",
    "        preference_data = []\n",
    "\n",
    "        for chosen_id, rejected_id in comparison_pairs:\n",
    "            chosen_conv = self.storage.load_conversation(chosen_id)\n",
    "            rejected_conv = self.storage.load_conversation(rejected_id)\n",
    "\n",
    "            if chosen_conv and rejected_conv:\n",
    "                # Extract comparable message pairs\n",
    "                chosen_msgs = chosen_conv[\"messages\"]\n",
    "                rejected_msgs = rejected_conv[\"messages\"]\n",
    "\n",
    "                # Find matching user prompts\n",
    "                for c_msg, r_msg in zip(chosen_msgs, rejected_msgs):\n",
    "                    if c_msg.get(\"role\") == \"user\" and r_msg.get(\"role\") == \"user\":\n",
    "                        # Get the corresponding responses\n",
    "                        c_idx = chosen_msgs.index(c_msg)\n",
    "                        r_idx = rejected_msgs.index(r_msg)\n",
    "\n",
    "                        if c_idx + 1 < len(chosen_msgs) and r_idx + 1 < len(rejected_msgs):\n",
    "                            data_point = {\n",
    "                                \"prompt\": c_msg.get(\"content\", \"\"),\n",
    "                                \"chosen\": chosen_msgs[c_idx + 1].get(\"content\", \"\"),\n",
    "                                \"rejected\": rejected_msgs[r_idx + 1].get(\"content\", \"\"),\n",
    "                                \"chosen_score\": chosen_conv.get(\"performance_metrics\", {}).get(\"quality_score\", 0),\n",
    "                                \"rejected_score\": rejected_conv.get(\"performance_metrics\", {}).get(\"quality_score\", 0)\n",
    "                            }\n",
    "                            preference_data.append(data_point)\n",
    "\n",
    "        return Dataset.from_list(preference_data)\n",
    "\n",
    "    def create_evaluation_dataset(self, test_size: float = 0.2) -> DatasetDict:\n",
    "        \"\"\"Create train/test split for evaluation\"\"\"\n",
    "\n",
    "        # Create full instruction dataset\n",
    "        full_dataset = self.create_instruction_dataset()\n",
    "\n",
    "        # Split into train/test\n",
    "        split_dataset = full_dataset.train_test_split(test_size=test_size, seed=42)\n",
    "\n",
    "        return DatasetDict({\n",
    "            \"train\": split_dataset[\"train\"],\n",
    "            \"test\": split_dataset[\"test\"]\n",
    "        })\n",
    "\n",
    "    def export_to_formats(self, dataset: Dataset, base_name: str = \"dataset\"):\n",
    "        \"\"\"Export dataset to multiple formats\"\"\"\n",
    "\n",
    "        export_paths = {}\n",
    "\n",
    "        # JSON\n",
    "        json_path = f\"{base_name}.json\"\n",
    "        dataset.to_json(json_path)\n",
    "        export_paths[\"json\"] = json_path\n",
    "\n",
    "        # CSV\n",
    "        csv_path = f\"{base_name}.csv\"\n",
    "        dataset.to_csv(csv_path)\n",
    "        export_paths[\"csv\"] = csv_path\n",
    "\n",
    "        # Parquet (efficient storage)\n",
    "        parquet_path = f\"{base_name}.parquet\"\n",
    "        dataset.to_parquet(parquet_path)\n",
    "        export_paths[\"parquet\"] = parquet_path\n",
    "\n",
    "        return export_paths\n",
    "\n",
    "print(\"\u2705 Training Dataset Builder initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_versioning_section"
   },
   "source": [
    "## \ud83d\udce6 Data Versioning with DagsHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "data_versioning_class",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "32ae2e4f-8457-4fa5-8ab3-a36bf30dca5f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Data Version Manager initialized\n"
     ]
    }
   ],
   "source": [
    "class DataVersionManager:\n",
    "    \"\"\"Manage data versioning with DagsHub and DVC\"\"\"\n",
    "\n",
    "    def __init__(self, repo_path: str = \".\"):\n",
    "        self.repo_path = Path(repo_path)\n",
    "        self.data_path = self.repo_path / \"data\"\n",
    "        self.data_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Initialize DVC if not already done\n",
    "        if not (self.repo_path / \".dvc\").exists():\n",
    "            os.system(\"dvc init\")\n",
    "\n",
    "    def version_dataset(self,\n",
    "                       dataset: Dataset,\n",
    "                       dataset_name: str,\n",
    "                       version: str,\n",
    "                       metadata: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"Version a dataset with DVC and track in MLflow\"\"\"\n",
    "\n",
    "        # Create version directory\n",
    "        version_path = self.data_path / dataset_name / version\n",
    "        version_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save dataset\n",
    "        dataset_file = version_path / \"dataset.parquet\"\n",
    "        dataset.save_to_disk(str(dataset_file))\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = metadata or {}\n",
    "        metadata.update({\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"version\": version,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"num_samples\": len(dataset),\n",
    "            \"features\": list(dataset.features.keys()) if hasattr(dataset, 'features') else [],\n",
    "            \"size_mb\": dataset_file.stat().st_size / (1024 * 1024)\n",
    "        })\n",
    "\n",
    "        metadata_file = version_path / \"metadata.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        # Track with DVC\n",
    "        os.system(f\"dvc add {dataset_file}\")\n",
    "        os.system(\"dvc push\")\n",
    "\n",
    "        # Log to MLflow\n",
    "        with mlflow.start_run(run_name=f\"dataset_{dataset_name}_v{version}\"):\n",
    "            mlflow.log_params(metadata)\n",
    "            mlflow.log_artifact(str(metadata_file))\n",
    "            mlflow.log_metric(\"dataset_size\", len(dataset))\n",
    "            mlflow.log_metric(\"file_size_mb\", metadata[\"size_mb\"])\n",
    "\n",
    "            # Log sample data\n",
    "            if len(dataset) > 0:\n",
    "                sample_file = version_path / \"sample.json\"\n",
    "                sample_data = dataset.select(range(min(5, len(dataset))))\n",
    "                sample_data.to_json(str(sample_file))\n",
    "                mlflow.log_artifact(str(sample_file))\n",
    "\n",
    "        return str(version_path)\n",
    "\n",
    "    def list_dataset_versions(self, dataset_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all versions of a dataset\"\"\"\n",
    "\n",
    "        dataset_path = self.data_path / dataset_name\n",
    "        versions = []\n",
    "\n",
    "        if dataset_path.exists():\n",
    "            for version_dir in dataset_path.iterdir():\n",
    "                if version_dir.is_dir():\n",
    "                    metadata_file = version_dir / \"metadata.json\"\n",
    "                    if metadata_file.exists():\n",
    "                        with open(metadata_file, 'r') as f:\n",
    "                            metadata = json.load(f)\n",
    "                            versions.append(metadata)\n",
    "\n",
    "        return sorted(versions, key=lambda x: x.get(\"created_at\", \"\"), reverse=True)\n",
    "\n",
    "    def load_dataset_version(self, dataset_name: str, version: str) -> Optional[Dataset]:\n",
    "        \"\"\"Load a specific version of a dataset\"\"\"\n",
    "\n",
    "        dataset_file = self.data_path / dataset_name / version / \"dataset.parquet\"\n",
    "\n",
    "        if dataset_file.exists():\n",
    "            return Dataset.from_parquet(str(dataset_file))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def create_dataset_changelog(self, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Create a changelog for dataset versions\"\"\"\n",
    "\n",
    "        versions = self.list_dataset_versions(dataset_name)\n",
    "\n",
    "        if versions:\n",
    "            df = pd.DataFrame(versions)\n",
    "            df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "            df = df.sort_values(\"created_at\", ascending=False)\n",
    "            return df\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"\u2705 Data Version Manager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_usage_section"
   },
   "source": [
    "## \ud83c\udfaf Example: Complete Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "generate_sample_conversations",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0ca274af-a3d5-4bf4-d406-aa4f1bcab85e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Sample conversation generator ready\n"
     ]
    }
   ],
   "source": [
    "# Generate sample conversations using different agent configurations\n",
    "def generate_sample_conversations(num_conversations: int = 10) -> List[Dict]:\n",
    "    \"\"\"Generate sample conversations for demonstration\"\"\"\n",
    "\n",
    "    agent_configs = [\n",
    "        {\n",
    "            \"name\": \"helpful_assistant\",\n",
    "            \"system_prompt\": \"You are a helpful assistant.\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 150\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"technical_expert\",\n",
    "            \"system_prompt\": \"You are a technical expert providing detailed explanations.\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 200\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"creative_writer\",\n",
    "            \"system_prompt\": \"You are a creative writer with a vivid imagination.\",\n",
    "            \"temperature\": 0.9,\n",
    "            \"max_tokens\": 250\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    test_prompts = [\n",
    "        \"How do I make a good first impression?\",\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"What's the best way to learn programming?\",\n",
    "        \"How can I improve my productivity?\",\n",
    "        \"Tell me about climate change solutions.\"\n",
    "    ]\n",
    "\n",
    "    conversations = []\n",
    "\n",
    "    for i in range(num_conversations):\n",
    "        # Select random configuration and prompt\n",
    "        config = np.random.choice(agent_configs)\n",
    "        prompt = np.random.choice(test_prompts)\n",
    "\n",
    "        # Generate conversation\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": config[\"system_prompt\"]},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=config[\"temperature\"],\n",
    "                max_tokens=config[\"max_tokens\"]\n",
    "            )\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt, \"tokens\": len(prompt.split())},\n",
    "                {\"role\": \"assistant\", \"content\": response.choices[0].message.content,\n",
    "                 \"tokens\": response.usage.completion_tokens}\n",
    "            ]\n",
    "\n",
    "            # Add performance metrics\n",
    "            performance_metrics = {\n",
    "                \"response_time\": np.random.uniform(0.5, 2.0),\n",
    "                \"quality_score\": np.random.uniform(0.6, 1.0),\n",
    "                \"total_tokens\": response.usage.total_tokens,\n",
    "                \"completion_tokens\": response.usage.completion_tokens\n",
    "            }\n",
    "\n",
    "            conversations.append({\n",
    "                \"messages\": messages,\n",
    "                \"agent_config\": config,\n",
    "                \"performance_metrics\": performance_metrics,\n",
    "                \"tags\": [\"sample\", config[\"name\"], f\"prompt_{test_prompts.index(prompt)}\"]\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating conversation {i}: {e}\")\n",
    "\n",
    "    return conversations\n",
    "\n",
    "print(\"\u2705 Sample conversation generator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "run_data_pipeline",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802,
     "referenced_widgets": [
      "ce5d186f39ad41ffa357ea82d6755ac5",
      "0824b7bf269e49d1a1708f9df7809724",
      "24a43a0025d8440e8123c9dd382475e7",
      "d84f2215664d40279ea277b18167a2c4",
      "3219f27ba0af40b29a0e740574242b9d",
      "ecc786cf89fb40d48d7cb967f837139d",
      "baab0d424a654590baa191b92f3a4e79",
      "94d940b721874464abd3a3f57173d431",
      "9ed76a46722f425cb10b2d079fddb9fe",
      "eb6ad4b396b54035ba6ab918865665e2",
      "546ca5c5415d459eb681f2922c418392",
      "cc0cfcf72a5745868edfaa21fab175ef",
      "815d9ba7726a4bc9b3b14dd7aaa63714",
      "ee1af7bf21874348b59033a91eb37c34",
      "b1ec8cc886984816b2969e1012b37291",
      "1787f5cc60754074be61d4eef9e2f6ed",
      "a9fa630437584dc58f12146d93117d46",
      "9c2f4ccc7e5d4beab22845e339685209",
      "7904b10018804c7da239292684983388",
      "cb5a9de3b6b7486c9bac18c7abe3b236",
      "fb28b89aa7bf4471ac9805ca73d6ade9",
      "9981c78aa1064b698dc8b7f97ae16f96",
      "2044cb2b420447f78fa95bbe39710691",
      "37a4941d8e3f45c78c14f371493ded63",
      "edb5422a42ec4529a0d235047abb27c8",
      "3467f758b6a944afb0455f0fc1857757",
      "e1b645e0183c48338c65748c22fabb86",
      "b8ed2d75b92844988402831c01806c85",
      "22c0d4b91bbb487ea3fec80c2f3ae52d",
      "a9ba2f853da04bd480a74ee536dda0bb",
      "f72c3aa1b5b24858a8e7e76408a34f69",
      "f2acdbe01dbc47bbb25e367a0fbaba19",
      "faee2f165fd342159424fae5f24b6001",
      "d25525c2ec9a42b0bf10518aab9db1ea",
      "2faa25aca80a413c9ebc0384b3fdc93c",
      "20e1f626a5bb4eaa991474035b30da86",
      "7cc852233d4940d0823ee42cac836e92",
      "18e7b1a47c4a4f97919defbdae1023d1",
      "38831e9e4e004e5699c013b17e5e3edb",
      "8be65d3dbbdd478f904b8af98133d673",
      "1a15179e1b7e4e0f8b783c3d1414ff21",
      "8594f93f2ba74f498fe5441b213a5d6c",
      "d4012b3c181949d5b45a764ea5b5c1fc",
      "0a9285b0c58c48aa95160700d816d961",
      "c1ee6d57bc634f7c8a35ce515c9da67e",
      "7a3e4368578a448090b7c29b8d055c6b",
      "82bd871695f7404ba7763722848178fb",
      "50bc9f9b40a2437c904d3602523f5e01",
      "d3d76607aaa24e97b133a1b6f33a8447",
      "428da44733eb4c9d90c58a9511db0e3f",
      "3f51d3b02aca427b976d5e2e874f8223",
      "1954cb5bb6054bb1a61c7edc54225dd4",
      "0e8b8a74a6d948158b833b08240d9e27",
      "f0dc40cd99434f6ca098d7c74a63dbd5",
      "593d816eb0b34dd3b4817de215405704"
     ]
    },
    "outputId": "292a831e-8720-4d83-ee71-ac7797fd4fef"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\ude80 Running Complete Data Pipeline...\n",
      "\n",
      "1\ufe0f\u20e3 Generating sample conversations...\n",
      "   \u2705 Stored 20 conversations\n",
      "\n",
      "2\ufe0f\u20e3 Conversation Statistics:\n",
      "   \u2022 total_conversations: 20\n",
      "   \u2022 total_messages: 40\n",
      "   \u2022 avg_messages_per_conversation: 2.0\n",
      "   \u2022 total_tokens: 4394\n",
      "   \u2022 avg_tokens_per_conversation: 219.7\n",
      "   \u2022 unique_agent_configs: 3\n",
      "   \u2022 date_range: {'start': '2025-08-13T18:52:27.588840', 'end': '2025-08-13T18:52:27.593961'}\n",
      "\n",
      "3\ufe0f\u20e3 Creating training datasets...\n",
      "   \u2705 Created instruction dataset with 15 samples\n",
      "   \u2705 Created train/test split: 12/3 samples\n",
      "\n",
      "4\ufe0f\u20e3 Versioning datasets with DagsHub...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce5d186f39ad41ffa357ea82d6755ac5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc0cfcf72a5745868edfaa21fab175ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83c\udfc3 View run dataset_agent_instructions_v1.0.0 at: https://dagshub.com/mpaul/conversation_and_datasets.mlflow/#/experiments/0/runs/051eab615cfc4d36a20aa14ed09bb35d\n",
      "\ud83e\uddea View experiment at: https://dagshub.com/mpaul/conversation_and_datasets.mlflow/#/experiments/0\n",
      "   \u2705 Dataset versioned at: data/agent_instructions/1.0.0\n",
      "\n",
      "5\ufe0f\u20e3 Exporting datasets...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2044cb2b420447f78fa95bbe39710691"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d25525c2ec9a42b0bf10518aab9db1ea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1ee6d57bc634f7c8a35ce515c9da67e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   \u2705 Exported to json: agent_dataset.json\n",
      "   \u2705 Exported to csv: agent_dataset.csv\n",
      "   \u2705 Exported to parquet: agent_dataset.parquet\n",
      "\n",
      "6\ufe0f\u20e3 Logging to MLflow...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2025/08/13 18:52:32 INFO mlflow.tracking.fluent: Experiment with name 'conversation_data_management' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   \u2705 Logged to MLflow run: c875fe02418e407c8b2d3a5432edeec3\n",
      "   \ud83d\udd17 View at: https://dagshub.com/mpaul/conversation_and_datasets.mlflow/#/experiments/0/runs/c875fe02418e407c8b2d3a5432edeec3\n",
      "\ud83c\udfc3 View run data_pipeline_run at: https://dagshub.com/mpaul/conversation_and_datasets.mlflow/#/experiments/1/runs/c875fe02418e407c8b2d3a5432edeec3\n",
      "\ud83e\uddea View experiment at: https://dagshub.com/mpaul/conversation_and_datasets.mlflow/#/experiments/1\n",
      "\n",
      "\u2728 Data Pipeline Complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "conv_storage = ConversationStorage()\n",
    "dataset_builder = TrainingDatasetBuilder(conv_storage)\n",
    "version_manager = DataVersionManager()\n",
    "\n",
    "print(\"\ud83d\ude80 Running Complete Data Pipeline...\\n\")\n",
    "\n",
    "# Step 1: Generate and store conversations\n",
    "print(\"1\ufe0f\u20e3 Generating sample conversations...\")\n",
    "sample_conversations = generate_sample_conversations(num_conversations=20)\n",
    "conversation_ids = conv_storage.batch_store_conversations(sample_conversations)\n",
    "print(f\"   \u2705 Stored {len(conversation_ids)} conversations\")\n",
    "\n",
    "# Step 2: Display statistics\n",
    "print(\"\\n2\ufe0f\u20e3 Conversation Statistics:\")\n",
    "stats = conv_storage.get_statistics()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   \u2022 {key}: {value}\")\n",
    "\n",
    "# Step 3: Create training datasets\n",
    "print(\"\\n3\ufe0f\u20e3 Creating training datasets...\")\n",
    "\n",
    "# Instruction dataset\n",
    "instruction_dataset = dataset_builder.create_instruction_dataset(format_type=\"alpaca\")\n",
    "print(f\"   \u2705 Created instruction dataset with {len(instruction_dataset)} samples\")\n",
    "\n",
    "# Evaluation split\n",
    "eval_datasets = dataset_builder.create_evaluation_dataset(test_size=0.2)\n",
    "print(f\"   \u2705 Created train/test split: {len(eval_datasets['train'])}/{len(eval_datasets['test'])} samples\")\n",
    "\n",
    "# Step 4: Version datasets\n",
    "print(\"\\n4\ufe0f\u20e3 Versioning datasets with DagsHub...\")\n",
    "version_path = version_manager.version_dataset(\n",
    "    dataset=instruction_dataset,\n",
    "    dataset_name=\"agent_instructions\",\n",
    "    version=\"1.0.0\",\n",
    "    metadata={\n",
    "        \"description\": \"Agent instruction-following dataset\",\n",
    "        \"source\": \"Generated from agent conversations\",\n",
    "        \"quality_threshold\": 0.7\n",
    "    }\n",
    ")\n",
    "print(f\"   \u2705 Dataset versioned at: {version_path}\")\n",
    "\n",
    "# Step 5: Export to multiple formats\n",
    "print(\"\\n5\ufe0f\u20e3 Exporting datasets...\")\n",
    "export_paths = dataset_builder.export_to_formats(instruction_dataset, \"agent_dataset\")\n",
    "for format_type, path in export_paths.items():\n",
    "    print(f\"   \u2705 Exported to {format_type}: {path}\")\n",
    "\n",
    "# Step 6: Log to MLflow\n",
    "print(\"\\n6\ufe0f\u20e3 Logging to MLflow...\")\n",
    "mlflow.set_experiment(\"conversation_data_management\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"data_pipeline_run\") as run:\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"total_conversations\": stats[\"total_conversations\"],\n",
    "        \"total_messages\": stats[\"total_messages\"],\n",
    "        \"avg_messages_per_conversation\": stats[\"avg_messages_per_conversation\"],\n",
    "        \"dataset_size\": len(instruction_dataset),\n",
    "        \"train_size\": len(eval_datasets[\"train\"]),\n",
    "        \"test_size\": len(eval_datasets[\"test\"])\n",
    "    })\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"dataset_format\": \"alpaca\",\n",
    "        \"quality_threshold\": 0.7,\n",
    "        \"test_split_ratio\": 0.2,\n",
    "        \"num_agent_configs\": 3\n",
    "    })\n",
    "\n",
    "    # Log artifacts\n",
    "    for path in export_paths.values():\n",
    "        if os.path.exists(path):\n",
    "            mlflow.log_artifact(path)\n",
    "\n",
    "    print(f\"   \u2705 Logged to MLflow run: {run.info.run_id}\")\n",
    "    print(f\"   \ud83d\udd17 View at: https://dagshub.com/{USER_NAME}/{REPO_NAME}.mlflow/#/experiments/0/runs/{run.info.run_id}\")\n",
    "\n",
    "print(\"\\n\u2728 Data Pipeline Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quality_analysis_section"
   },
   "source": [
    "## \ud83d\udcca Dataset Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dataset_quality_analyzer",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97fd84e4-c962-4fc8-d117-5a90509eb31e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcca Running Dataset Quality Analysis...\n",
      "\n",
      "Quality Report:\n",
      "    category                metric       value\n",
      "Text Quality            avg_length 1084.733333\n",
      "Text Quality            std_length  260.935360\n",
      "Text Quality             avg_words  165.000000\n",
      "Text Quality          unique_words  927.000000\n",
      "Text Quality       empty_responses    0.000000\n",
      "Text Quality   duplicate_responses    0.000000\n",
      "   Diversity instruction_diversity    0.333333\n",
      "   Diversity  unique_agent_configs    3.000000\n",
      "   Diversity         total_samples   15.000000\n",
      "\ud83c\udfc3 View run dataset_quality_analysis at: https://dagshub.com/mpaul/conversation_and_datasets.mlflow/#/experiments/1/runs/79df02be53cc4814b6888e983750f42e\n",
      "\ud83e\uddea View experiment at: https://dagshub.com/mpaul/conversation_and_datasets.mlflow/#/experiments/1\n",
      "\n",
      "\u2705 Quality analysis complete and logged to MLflow\n"
     ]
    }
   ],
   "source": [
    "class DatasetQualityAnalyzer:\n",
    "    \"\"\"Analyze quality of training datasets\"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def analyze_text_quality(self, text_column: str = \"output\") -> Dict[str, Any]:\n",
    "        \"\"\"Analyze text quality metrics\"\"\"\n",
    "\n",
    "        texts = self.dataset[text_column]\n",
    "\n",
    "        metrics = {\n",
    "            \"avg_length\": np.mean([len(t) for t in texts]),\n",
    "            \"std_length\": np.std([len(t) for t in texts]),\n",
    "            \"avg_words\": np.mean([len(t.split()) for t in texts]),\n",
    "            \"unique_words\": len(set(\" \".join(texts).split())),\n",
    "            \"empty_responses\": sum(1 for t in texts if not t.strip()),\n",
    "            \"duplicate_responses\": len(texts) - len(set(texts))\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def analyze_diversity(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze dataset diversity\"\"\"\n",
    "\n",
    "        # Calculate diversity metrics\n",
    "        if \"instruction\" in self.dataset.features:\n",
    "            instructions = self.dataset[\"instruction\"]\n",
    "            unique_instructions = len(set(instructions))\n",
    "            instruction_diversity = unique_instructions / len(instructions)\n",
    "        else:\n",
    "            instruction_diversity = 0\n",
    "\n",
    "        if \"agent_config\" in self.dataset.features:\n",
    "            configs = self.dataset[\"agent_config\"]\n",
    "            unique_configs = len(set(configs))\n",
    "        else:\n",
    "            unique_configs = 0\n",
    "\n",
    "        return {\n",
    "            \"instruction_diversity\": instruction_diversity,\n",
    "            \"unique_agent_configs\": unique_configs,\n",
    "            \"total_samples\": len(self.dataset)\n",
    "        }\n",
    "\n",
    "    def generate_quality_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate comprehensive quality report\"\"\"\n",
    "\n",
    "        report_data = []\n",
    "\n",
    "        # Text quality\n",
    "        if \"output\" in self.dataset.features:\n",
    "            text_metrics = self.analyze_text_quality(\"output\")\n",
    "            for key, value in text_metrics.items():\n",
    "                report_data.append({\n",
    "                    \"category\": \"Text Quality\",\n",
    "                    \"metric\": key,\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "        # Diversity\n",
    "        diversity_metrics = self.analyze_diversity()\n",
    "        for key, value in diversity_metrics.items():\n",
    "            report_data.append({\n",
    "                \"category\": \"Diversity\",\n",
    "                \"metric\": key,\n",
    "                \"value\": value\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(report_data)\n",
    "\n",
    "# Run quality analysis\n",
    "if 'instruction_dataset' in locals():\n",
    "    print(\"\ud83d\udcca Running Dataset Quality Analysis...\\n\")\n",
    "\n",
    "    analyzer = DatasetQualityAnalyzer(instruction_dataset)\n",
    "    quality_report = analyzer.generate_quality_report()\n",
    "\n",
    "    print(\"Quality Report:\")\n",
    "    print(quality_report.to_string(index=False))\n",
    "\n",
    "    # Save report\n",
    "    quality_report.to_csv(\"dataset_quality_report.csv\", index=False)\n",
    "\n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_name=\"dataset_quality_analysis\"):\n",
    "        mlflow.log_artifact(\"dataset_quality_report.csv\")\n",
    "\n",
    "        for _, row in quality_report.iterrows():\n",
    "            mlflow.log_metric(f\"{row['category']}_{row['metric']}\", row['value'])\n",
    "\n",
    "    print(\"\\n\u2705 Quality analysis complete and logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "integration_section"
   },
   "source": [
    "## \ud83d\udd04 Integration with Agent Performance Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "performance_data_integration",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "78e67e6c-e087-4102-b217-fbaa1864ea59"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udd04 Integrating with Performance Tracking...\n",
      "\n",
      "\u2705 Linked 0 experiment runs\n",
      "\u2705 Created performance dataset with 15 high-quality samples\n",
      "\n",
      "\ud83c\udf89 Integration complete!\n"
     ]
    }
   ],
   "source": [
    "class PerformanceDataIntegration:\n",
    "    \"\"\"Integrate conversation data with performance tracking\"\"\"\n",
    "\n",
    "    def __init__(self, conversation_storage: ConversationStorage):\n",
    "        self.storage = conversation_storage\n",
    "        self.mlflow_client = MlflowClient()\n",
    "\n",
    "    def link_conversations_to_experiments(self, experiment_name: str = \"Agent_Performance_Tracking_Comprehensive\"):\n",
    "        \"\"\"Link stored conversations to MLflow experiments\"\"\"\n",
    "\n",
    "        # Get experiment\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "        if experiment:\n",
    "            # Get all runs from the experiment\n",
    "            runs = self.mlflow_client.search_runs(\n",
    "                experiment_ids=[experiment.experiment_id],\n",
    "                order_by=[\"start_time DESC\"]\n",
    "            )\n",
    "\n",
    "            linked_data = []\n",
    "\n",
    "            for run in runs:\n",
    "                # Extract agent config from run\n",
    "                agent_config = {\n",
    "                    \"name\": run.data.params.get(\"agent_name\", \"unknown\"),\n",
    "                    \"temperature\": float(run.data.params.get(\"temperature\", 0.7)),\n",
    "                    \"max_tokens\": int(run.data.params.get(\"max_tokens\", 150))\n",
    "                }\n",
    "\n",
    "                # Extract performance metrics\n",
    "                performance_metrics = {\n",
    "                    \"overall_score\": run.data.metrics.get(\"overall_performance_score\", 0),\n",
    "                    \"response_time\": run.data.metrics.get(\"avg_response_time\", 0),\n",
    "                    \"tokens_used\": run.data.metrics.get(\"avg_tokens_per_response\", 0)\n",
    "                }\n",
    "\n",
    "                linked_data.append({\n",
    "                    \"run_id\": run.info.run_id,\n",
    "                    \"experiment_id\": experiment.experiment_id,\n",
    "                    \"agent_config\": agent_config,\n",
    "                    \"performance_metrics\": performance_metrics,\n",
    "                    \"timestamp\": run.info.start_time\n",
    "                })\n",
    "\n",
    "            return linked_data\n",
    "\n",
    "        return []\n",
    "\n",
    "    def create_performance_based_dataset(self, min_performance_score: float = 0.7) -> Dataset:\n",
    "        \"\"\"Create dataset from high-performing agent interactions\"\"\"\n",
    "\n",
    "        # Filter conversations by performance\n",
    "        high_quality_conversations = [\n",
    "            conv for conv in self.storage.conversations\n",
    "            if conv.get(\"performance_metrics\", {}).get(\"quality_score\", 0) >= min_performance_score\n",
    "        ]\n",
    "\n",
    "        # Create dataset\n",
    "        dataset_items = []\n",
    "        for conv in high_quality_conversations:\n",
    "            for msg_pair in zip(conv[\"messages\"][::2], conv[\"messages\"][1::2]):\n",
    "                if len(msg_pair) == 2:\n",
    "                    dataset_items.append({\n",
    "                        \"instruction\": msg_pair[0].get(\"content\", \"\"),\n",
    "                        \"response\": msg_pair[1].get(\"content\", \"\"),\n",
    "                        \"performance_score\": conv[\"performance_metrics\"].get(\"quality_score\", 0),\n",
    "                        \"agent_name\": conv[\"agent_config\"].get(\"name\", \"unknown\")\n",
    "                    })\n",
    "\n",
    "        return Dataset.from_list(dataset_items)\n",
    "\n",
    "# Example integration\n",
    "print(\"\ud83d\udd04 Integrating with Performance Tracking...\\n\")\n",
    "\n",
    "integrator = PerformanceDataIntegration(conv_storage)\n",
    "\n",
    "# Link to existing experiments\n",
    "linked_data = integrator.link_conversations_to_experiments()\n",
    "print(f\"\u2705 Linked {len(linked_data)} experiment runs\")\n",
    "\n",
    "# Create performance-based dataset\n",
    "performance_dataset = integrator.create_performance_based_dataset(min_performance_score=0.7)\n",
    "print(f\"\u2705 Created performance dataset with {len(performance_dataset)} high-quality samples\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 Integration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_section"
   },
   "source": [
    "## \ud83d\udccb Summary & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "summary_output",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "08d88bfb-2e17-4a31-b061-1f07693c7f5b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83c\udfaf CONVERSATION DATA & TRAINING DATASET MANAGEMENT SUMMARY\n",
      "============================================================\n",
      "\n",
      "\u2705 What we implemented:\n",
      "  \ud83d\udcda Comprehensive conversation storage system\n",
      "  \ud83c\udfd7\ufe0f Training dataset builder with multiple formats\n",
      "  \ud83d\udce6 Data versioning with DagsHub and DVC\n",
      "  \ud83d\udcca Dataset quality analysis tools\n",
      "  \ud83d\udd04 Integration with performance tracking\n",
      "\n",
      "\ud83d\udd11 Key Features:\n",
      "  \u2022 Store and version conversation histories\n",
      "  \u2022 Create instruction-following datasets (Alpaca, ChatGPT formats)\n",
      "  \u2022 Build preference datasets for RLHF\n",
      "  \u2022 Track dataset versions with MLflow\n",
      "  \u2022 Analyze dataset quality and diversity\n",
      "  \u2022 Export to multiple formats (JSON, CSV, Parquet)\n",
      "\n",
      "\ud83d\udcc8 Best Practices:\n",
      "  1. Version all datasets with meaningful metadata\n",
      "  2. Filter conversations by quality scores\n",
      "  3. Maintain train/test splits for evaluation\n",
      "  4. Track dataset lineage and transformations\n",
      "  5. Regular quality analysis and monitoring\n",
      "\n",
      "\ud83d\udd17 DagsHub Features Used:\n",
      "  \u2022 Data versioning with DVC integration\n",
      "  \u2022 MLflow experiment and dataset tracking\n",
      "  \u2022 Artifact storage and management\n",
      "  \u2022 Collaborative data pipeline development\n",
      "\n",
      "\ud83c\udf10 View your data at: https://dagshub.com/mpaul/conversation_and_datasets\n",
      "\ud83d\udcca MLflow tracking: https://dagshub.com/mpaul/conversation_and_datasets/experiments\n",
      "\n",
      "\ud83c\udf89 Data Management Pipeline Complete! \ud83c\udf89\n",
      "\n",
      "You now have a complete system for:\n",
      "  1. Versioning agent prompts and configurations \u2705\n",
      "  2. Tracking agent performance experiments \u2705\n",
      "  3. Storing conversation data and training datasets \u2705\n"
     ]
    }
   ],
   "source": [
    "print(\"\ud83c\udfaf CONVERSATION DATA & TRAINING DATASET MANAGEMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\u2705 What we implemented:\")\n",
    "print(\"  \ud83d\udcda Comprehensive conversation storage system\")\n",
    "print(\"  \ud83c\udfd7\ufe0f Training dataset builder with multiple formats\")\n",
    "print(\"  \ud83d\udce6 Data versioning with DagsHub and DVC\")\n",
    "print(\"  \ud83d\udcca Dataset quality analysis tools\")\n",
    "print(\"  \ud83d\udd04 Integration with performance tracking\")\n",
    "\n",
    "print(\"\\n\ud83d\udd11 Key Features:\")\n",
    "print(\"  \u2022 Store and version conversation histories\")\n",
    "print(\"  \u2022 Create instruction-following datasets (Alpaca, ChatGPT formats)\")\n",
    "print(\"  \u2022 Build preference datasets for RLHF\")\n",
    "print(\"  \u2022 Track dataset versions with MLflow\")\n",
    "print(\"  \u2022 Analyze dataset quality and diversity\")\n",
    "print(\"  \u2022 Export to multiple formats (JSON, CSV, Parquet)\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Best Practices:\")\n",
    "print(\"  1. Version all datasets with meaningful metadata\")\n",
    "print(\"  2. Filter conversations by quality scores\")\n",
    "print(\"  3. Maintain train/test splits for evaluation\")\n",
    "print(\"  4. Track dataset lineage and transformations\")\n",
    "print(\"  5. Regular quality analysis and monitoring\")\n",
    "\n",
    "print(\"\\n\ud83d\udd17 DagsHub Features Used:\")\n",
    "print(\"  \u2022 Data versioning with DVC integration\")\n",
    "print(\"  \u2022 MLflow experiment and dataset tracking\")\n",
    "print(\"  \u2022 Artifact storage and management\")\n",
    "print(\"  \u2022 Collaborative data pipeline development\")\n",
    "\n",
    "print(f\"\\n\ud83c\udf10 View your data at: https://dagshub.com/{USER_NAME}/{REPO_NAME}\")\n",
    "print(f\"\ud83d\udcca MLflow tracking: https://dagshub.com/{USER_NAME}/{REPO_NAME}/experiments\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 Data Management Pipeline Complete! \ud83c\udf89\")\n",
    "print(\"\\nYou now have a complete system for:\")\n",
    "print(\"  1. Versioning agent prompts and configurations \u2705\")\n",
    "print(\"  2. Tracking agent performance experiments \u2705\")\n",
    "print(\"  3. Storing conversation data and training datasets \u2705\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}